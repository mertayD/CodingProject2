<<<<<<< HEAD
---
title: "Vignette Title"
author: "Vignette Author"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


```{r data}
usethis::use_package("codingProject2")
#usethis::use_package("modeest")

data.name.vec <- c(
  "spam",
  "SAheart",
  "zip.train",
  "prostate",
  "ozone")
data.list <- list()
library(ElemStatLearn)
for(data.name in data.name.vec)
{
  data(list = data.name, package="ElemStatLearn")
  data.list[[data.name]] <- get(data.name)
}
str(data.list)
is.binary <- ElemStatLearn::zip.train[,1] %in% c(0,1)
data.list <- list(
  spam=list(
    label.vec=ifelse(ElemStatLearn::spam$spam=="spam", 1, 0),
    feature.mat=as.matrix(ElemStatLearn::spam[, 1:57])),
  SAheart=list(
    label.vec=ifelse(ElemStatLearn::SAheart$SAheart=="chd", 1, 0),
    feature.mat=as.matrix(ElemStatLearn::SAheart[, -ncol(ElemStatLearn::SAheart)])),
  zip.train=list(
    label.vec=ElemStatLearn::zip.train[is.binary,1],
    feature.mat=ElemStatLearn::zip.train[is.binary,-1]),
  prostate=list(
    label.vec=ElemStatLearn::prostate$lpsa,
    feature.mat=as.matrix(ElemStatLearn::prostate[, -c(9,10)])),
  ozone=list(
    label.vec=ElemStatLearn::ozone$ozone,
    feature.mat=as.matrix(ElemStatLearn::ozone[, -ncol(ElemStatLearn::ozone)]))
)

n.folds <- 4
resultsList <- list()
baseline <- list()
mean.loss.list <- list()
current.set.matrix <-

for(data.name in names(data.list))
{
  one.data.set <- data.list[[data.name]]
  set.seed(1)
  
  # print("This is one.data.set")
  # print(one.data.set)Show in New WindowClear OutputExpand/Collapse Output
â— Refer to functions with `codingProject2::fun()`
List of 5
 $ spam     :'data.frame':	4601 obs. of  58 variables:
  ..$ A.1 : num [1:4601] 0 0.21 0.06 0 0 0 0 0 0.15 0.06 ...
  ..$ A.2 : num [1:4601] 0.64 0.28 0 0 0 0 0 0 0 0.12 ...
  ..$ A.3 : num [1:4601] 0.64 0.5 0.71 0 0 0 0 0 0.46 0.77 ...
  ..$ A.4 : num [1:4601] 0 0 0 0 0 0 0 0 0 0 ...
  ..$ A.5 : num [1:4601] 0.32 0.14 1.23 0.63 0.63 1.85 1.92 1.88 0.61 0.19 ...
  ..$ A.6 : num [1:4601] 0 0.28 0.19 0 0 0 0 0 0 0.32 ...
  ..$ A.7 : num [1:4601] 0 0.21 0.19 0.31 0.31 0 0 0 0.3 0.38 ...
  ..$ A.8 : num [1:4601] 0 0.07 0.12 0.63 0.63 1.85 0 1.88 0 0 ...
  ..$ A.9 : num [1:4601] 0 0 0.64 0.31 0.31 0 0 0 0.92 0.06 ...
  ..$ A.10: num [1:4601] 0 0.94 0.25 0.63 0.63 0 0.64 0 0.76 0 ...
  ..$ A.11: num [1:4601] 0 0.21 0.38 0.31 0.31 0 0.96 0 0.76 0 ...
  ..$ A.12: num [1:4601] 0.64 0.79 0.45 0.31 0.31 0 1.28 0 0.92 0.64 ...
  ..$ A.13: num [1:4601] 0 0.65 0.12 0.31 0.31 0 0 0 0 0.25 ...
  ..$ A.14: num [1:4601] 0 0.21 0 0 0 0 0 0 0 0 ...
  ..$ A.15: num [1:4601] 0 0.14 1.75 0 0 0 0 0 0 0.12 ...
  ..$ A.16: num [1:4601] 0.32 0.14 0.06 0.31 0.31 0 0.96 0 0 0 ...
  ..$ A.17: num [1:4601] 0 0.07 0.06 0 0 0 0 0 0 0 ...
  ..$ A.18: num [1:4601] 1.29 0.28 1.03 0 0 0 0.32 0 0.15 0.12 ...
  ..$ A.19: num [1:4601] 1.93 3.47 1.36 3.18 3.18 0 3.85 0 1.23 1.67 ...
  ..$ A.20: num [1:4601] 0 0 0.32 0 0 0 0 0 3.53 0.06 ...
  ..$ A.21: num [1:4601] 0.96 1.59 0.51 0.31 0.31 0 0.64 0 2 0.71 ...
  ..$ A.22: num [1:4601] 0 0 0 0 0 0 0 0 0 0 ...
  ..$ A.23: num [1:4601] 0 0.43 1.16 0 0 0 0 0 0 0.19 ...
  ..$ A.24: num [1:4601] 0 0.43 0.06 0 0 0 0 0 0.15 0 ...
  ..$ A.25: num [1:4601] 0 0 0 0 0 0 0 0 0 0 ...
  ..$ A.26: num [1:4601] 0 0 0 0 0 0 0 0 0 0 ...
  ..$ A.27: num [1:4601] 0 0 0 0 0 0 0 0 0 0 ...
  ..$ A.28: num [1:4601] 0 0 0 0 0 0 0 0 0 0 ...
  ..$ A.29: num [1:4601] 0 0 0 0 0 0 0 0 0 0 ...
  ..$ A.30: num [1:4601] 0 0 0 0 0 0 0 0 0 0 ...
  ..$ A.31: num [1:4601] 0 0 0 0 0 0 0 0 0 0 ...
  ..$ A.32: num [1:4601] 0 0 0 0 0 0 0 0 0 0 ...
  ..$ A.33: num [1:4601] 0 0 0 0 0 0 0 0 0.15 0 ...
  ..$ A.34: num [1:4601] 0 0 0 0 0 0 0 0 0 0 ...
  ..$ A.35: num [1:4601] 0 0 0 0 0 0 0 0 0 0 ...
  ..$ A.36: num [1:4601] 0 0 0 0 0 0 0 0 0 0 ...
  ..$ A.37: num [1:4601] 0 0.07 0 0 0 0 0 0 0 0 ...
  ..$ A.38: num [1:4601] 0 0 0 0 0 0 0 0 0 0 ...
  ..$ A.39: num [1:4601] 0 0 0 0 0 0 0 0 0 0 ...
  ..$ A.40: num [1:4601] 0 0 0.06 0 0 0 0 0 0 0 ...
  ..$ A.41: num [1:4601] 0 0 0 0 0 0 0 0 0 0 ...
  ..$ A.42: num [1:4601] 0 0 0 0 0 0 0 0 0 0 ...
  ..$ A.43: num [1:4601] 0 0 0.12 0 0 0 0 0 0.3 0 ...
  ..$ A.44: num [1:4601] 0 0 0 0 0 0 0 0 0 0.06 ...
  ..$ A.45: num [1:4601] 0 0 0.06 0 0 0 0 0 0 0 ...
  ..$ A.46: num [1:4601] 0 0 0.06 0 0 0 0 0 0 0 ...
  ..$ A.47: num [1:4601] 0 0 0 0 0 0 0 0 0 0 ...
  ..$ A.48: num [1:4601] 0 0 0 0 0 0 0 0 0 0 ...
  ..$ A.49: num [1:4601] 0 0 0.01 0 0 0 0 0 0 0.04 ...
  ..$ A.50: num [1:4601] 0 0.132 0.143 0.137 0.135 0.223 0.054 0.206 0.271 0.03 ...
  ..$ A.51: num [1:4601] 0 0 0 0 0 0 0 0 0 0 ...
  ..$ A.52: num [1:4601] 0.778 0.372 0.276 0.137 0.135 0 0.164 0 0.181 0.244 ...
  ..$ A.53: num [1:4601] 0 0.18 0.184 0 0 0 0.054 0 0.203 0.081 ...
  ..$ A.54: num [1:4601] 0 0.048 0.01 0 0 0 0 0 0.022 0 ...
  ..$ A.55: num [1:4601] 3.76 5.11 9.82 3.54 3.54 ...
  ..$ A.56: int [1:4601] 61 101 485 40 40 15 4 11 445 43 ...
  ..$ A.57: int [1:4601] 278 1028 2259 191 191 54 112 49 1257 749 ...
  ..$ spam: Factor w/ 2 levels "email","spam": 2 2 2 2 2 2 2 2 2 2 ...
 $ SAheart  :'data.frame':	462 obs. of  10 variables:
  ..$ sbp      : int [1:462] 160 144 118 170 134 132 142 114 114 132 ...
  ..$ tobacco  : num [1:462] 12 0.01 0.08 7.5 13.6 6.2 4.05 4.08 0 0 ...
  ..$ ldl      : num [1:462] 5.73 4.41 3.48 6.41 3.5 6.47 3.38 4.59 3.83 5.8 ...
  ..$ adiposity: num [1:462] 23.1 28.6 32.3 38 27.8 ...
  ..$ famhist  : Factor w/ 2 levels "Absent","Present": 2 1 2 2 2 2 1 2 2 2 ...
  ..$ typea    : int [1:462] 49 55 52 51 60 62 59 62 49 69 ...
  ..$ obesity  : num [1:462] 25.3 28.9 29.1 32 26 ...
  ..$ alcohol  : num [1:462] 97.2 2.06 3.81 24.26 57.34 ...
  ..$ age      : int [1:462] 52 63 46 58 49 45 38 58 29 53 ...
  ..$ chd      : int [1:462] 1 1 0 1 1 0 0 1 0 1 ...
 $ zip.train: num [1:7291, 1:257] 6 5 4 7 3 6 3 1 0 1 ...
 $ prostate :'data.frame':	97 obs. of  10 variables:
  ..$ lcavol : num [1:97] -0.58 -0.994 -0.511 -1.204 0.751 ...
  ..$ lweight: num [1:97] 2.77 3.32 2.69 3.28 3.43 ...
  ..$ age    : int [1:97] 50 58 74 58 62 50 64 58 47 63 ...
  ..$ lbph   : num [1:97] -1.39 -1.39 -1.39 -1.39 -1.39 ...
  ..$ svi    : int [1:97] 0 0 0 0 0 0 0 0 0 0 ...
  ..$ lcp    : num [1:97] -1.39 -1.39 -1.39 -1.39 -1.39 ...
  ..$ gleason: int [1:97] 6 6 7 6 6 6 6 6 6 6 ...
  ..$ pgg45  : int [1:97] 0 0 20 0 0 0 0 0 0 0 ...
  ..$ lpsa   : num [1:97] -0.431 -0.163 -0.163 -0.163 0.372 ...
  ..$ train  : logi [1:97] TRUE TRUE TRUE TRUE TRUE TRUE ...
 $ ozone    :'data.frame':	111 obs. of  4 variables:
  ..$ ozone      : num [1:111] 41 36 12 18 23 19 8 16 11 14 ...
  ..$ radiation  : int [1:111] 190 118 149 313 299 99 19 256 290 274 ...
  ..$ temperature: int [1:111] 67 72 74 62 65 59 61 69 66 68 ...
  ..$ wind       : num [1:111] 7.4 8 12.6 11.5 8.6 13.8 20.1 9.7 9.2 10.9 ...
[1] 1
[1] 2
[1] 3
[1] 4
[1] 1
[1] 2
[1] 3
[1] 4
[1] 1
[1] 2
[1] 3
[1] 4
[1] 1
 Show Traceback
Error in matrix(0, ncol(X.scaled), max.iterations) : invalid 'ncol' value (too large or NA)
  # print(data.list[[data.name]])
  # print(names(data.list))
  
  #print(one.data.set$feature.mat[1:nrow(one.data.set$feature.mat)])
  
  #print(one.data.set[1:nrow(one.data.set$feature.mat), -1])
  
  # variables for function
  X.mat <- one.data.set$feature.mat
  y.vec <- one.data.set$label.vec
  fold.vec <- sample(rep(1:n.folds, l=nrow(X.mat)))
  max.iterations <- 30

  result.mat.list <- list()
  
  for(test.fold in 1:n.folds)
  {
    is.test <- fold.vec == test.fold
    is.train <- !is.test
    X.train <- X.mat[is.train,]
    y.train <- y.vec[is.train]
    
    fold.vec.train <- sample(rep(1:n.folds, l=nrow(X.train)))
    
    print(test.fold)
    # print(y.train)
    # print(fold.vec)
    
    if(data.name == "prostate" || data.name == "ozone")
    {  
       #For each train/test split, to show that your algorithm is actually learning something     non-trivial from          the inputs/features, compute a baseline predictor that ignores the inputs/features.
       #Regression: the mean of the training labels/outputs.
    
       baseline <- mean(y.train)
    
       # print(data.name)
       # print(data.list[[data.name]])
    
       resultES <- codingProject2::LMSquareLossEarlyStoppingCV(X.train,
                                               y.train,
                                               fold.vec.train,
                                               max.iterations)
       resultREG <- codingProject2::LMSquareLossIterations(X.train,
                                           y.train,
                                           fold.vec.train,
                                           max.iterations)
       
       str(resultES)
       str(resultREG)
    
       pred.prob.list[test.fold] <- list(
         earlyStopping=resultES$predict(data.set$features[is.test,]),
         L2regularized=resultREG$predict(data.set$features[is.test,]),
         baseline=rep(baseline, sum(is.test)))
       
         # For each data set, compute a 4 x 3 matrix of mean test loss values:
         # each of the four rows are for a specific test set,
         # the first column is for the early stopping predictor,
         # the second column is for the L2 regularized predictor,
         # the third column is for the baseline/un-informed predictor.
  
         # mat <- cbind(resultES$penalty.vec, result$penalty.vec)
       
       
       
 #      str(pred.prob.list)
  #     test.fold.result.list <- list()
   #    for(algo in names(pred.prob.list))
    #   {
     #    pred.prob.vec <- pred.prob.list[[algo]]
      #   pred.class.vec <- ifelse(pred.prob.vec > 0.5, 1, 0)
       #  test.fold.result.list[[algo]] <- mean(data.set$labels[is.test] != #pred.class.vec)
 #      }
    }
  }
  

  # For spam SAheart and train
  
#  else
#  {
    
    #For each train/test split, to show that your algorithm is actually learning something non-trivial from the inputs/features, compute a baseline predictor that ignores the inputs/features.
    # Binary classification: the most frequent class/label/output in the training data.
    
    #prdictionBase <- mfv(fold.vec)
    #baseline.append(mfv(fold.vec))
    
#    resultES <- LMLogisticLossEarlyStoppingCV(X.mat,
#                                            y.vec,
#                                            fold.vec,
                                            #max.iterations)
#    result <- LMLogisticLossIterations(X.mat,
#                                     y.vec,
#                                     fold.vec,
#                                     max.iterations)
    
#    resultList.append(resultES)
#    resultList.append(result)
    
    
#  }
    
  # For each data set, compute a 4 x 3 matrix of mean test loss values:
    # each of the four rows are for a specific test set,
    # the first column is for the early stopping predictor,
    # the second column is for the L2 regularized predictor,
    # the third column is for the baseline/un-informed predictor.
  
  #mat <- cbind(resultES$penalty.vec, result$penalty.vec)
  
  # List is (LMLogisticLossEarlyStoppingCV, LMLogisticLossIterations for spam SAheart and then train, 
  # Then  LMSquareLossEarlyStoppingCV LMSquareLossIterations for prostate and ozone)

}
```

## Data set 1: spam

### Matrix of loss values

```{r, result = "asis"}
#print out and/or plot the matrix of loss values
pander::pandoc.table(resultsList[1]$train.loss.mat)
pander::pandoc.table(resultsList[2]$train.loss.mat)
```

comment on difference between NN and baseline.

### Train/validation loss plot

```{r}
#plot the two loss functions.
ggplot(resultsList[1]$train.loss.vec)
ggplot(resultsList[1]$validation.loss.vec)

ggplot(resultsList[2]$train.loss.vec)
ggplot(resultsList[2]$validation.loss.vec)
```

What are the optimal regularization parameters?

```{r}
# selected 
```

## Data set 2: SAheart

### Matrix of loss values

```{r, result = "asis"}
#print out and/or plot the matrix.
pander::pandoc.table(resultsList[3]$train.loss.mat)
pander::pandoc.table(resultsList[4]$train.loss.mat)
```

comment on difference between NN and baseline.

### Train/validation loss plot

```{r}
#plot the two loss functions.
ggplot(resultsList[3]$train.loss.vec)
ggplot(resultsList[3]$validation.loss.vec)

ggplot(resultsList[4]$train.loss.vec)
ggplot(resultsList[4]$validation.loss.vec)
```

What are the optimal regularization parameters?

```{r}
# selected
```

## Data set 3: zip.train

### Matrix of loss values

```{r, result = "asis"}
#print out and/or plot the matrix.
pander::pandoc.table(resultsList[5]$train.loss.mat)
pander::pandoc.table(resultsList[6]$train.loss.mat)
```

comment on difference between NN and baseline.

### Train/validation loss plot

```{r}
#plot the two loss functions.
#plot the two loss functions.
ggplot(resultsList[5]$train.loss.vec)
ggplot(resultsList[5]$validation.loss.vec)

ggplot(resultsList[6]$train.loss.vec)
ggplot(resultsList[6]$validation.loss.vec)
```

What are the optimal regularization parameters?

```{r}
# selected
```

## Data set 4: prostate 

### Matrix of loss values

```{r, result = "asis"}
#print out and/or plot the matrix.
pander::pandoc.table(resultsList[7]$train.loss.mat)
pander::pandoc.table(resultsList[8]$train.loss.mat)
```

comment on difference between NN and baseline.

### Train/validation loss plot

```{r}
#plot the two loss functions.
ggplot(resultsList[7]$train.loss.vec)
ggplot(resultsList[7]$validation.loss.vec)

ggplot(resultsList[8]$train.loss.vec)
ggplot(resultsList[8]$validation.loss.vec)
```

What are the optimal regularization parameters?

```{r}
# selected
```

## Data set 5: ozone 

### Matrix of loss values

```{r, result = "asis"}
#print out and/or plot the matrix.
pander::pandoc.table(resultsList[9]$train.loss.mat)
pander::pandoc.table(resultsList[10]$train.loss.mat)
```

comment on difference between NN and baseline.

### Train/validation loss plot

```{r}
#plot the two loss functions.
ggplot(resultsList[9]$train.loss.vec)
ggplot(resultsList[9]$validation.loss.vec)

ggplot(resultsList[10]$train.loss.vec)
ggplot(resultsList[10]$validation.loss.vec)
```

What are the optimal regularization parameters?

```{r}
# selected
```
